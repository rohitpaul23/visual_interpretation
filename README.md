# visual_interpretation

This repository contain files representing different ways to visualize what parts of the image the model is paying attention to when deciding the class of the image. 
The files are:
1. CAM: Used Class Activation Map(CAM) and Gradient weighted Class Activation Map(GradCAM) to visualize and interpret on Cat vs Dog dataset
2. Saliency: Used saliency map to visualize and interpret on Cat image

** The projects are made by taking help from deeplearning.ai course TensorFlow-Advanced-Techniques-Specialization
